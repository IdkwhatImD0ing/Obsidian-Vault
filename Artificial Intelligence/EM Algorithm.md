### Expectation Maximization/Estimation Modification (EM)

EM is not a singular algorithm but a framework that can be applied in multiple contexts. Its general idea revolves around:

1. **Training (Parameter Estimation or Maximum Likelihood Estimation)**
   - Aim: Find a model with parameters that maximize probability.
   - Process: Update assignment, i.e., assign data to clusters.

2. **Maximize (Classification, Prediction, Expectation Maximization)**
   - Aim: Predict a class \( C \) where probability is maximum.
   - Process: Update means, i.e., adjust clusters.

## Maximum Likelihood Estimation (MLE)

### Maximum Likelihood Principle

- **Given**: Data \( X \)
- **Assumption**: Data is generated by some model \( f(O) \)
   - \( F \): Model
   - \( O \): Model parameters
- **Objective**: Estimate \( P(X|O) \), the probability that our model \( f \) with parameters \( O \) generated the data.
- **Method**: Find the model parameters \( O \) that maximize \( P(X|O) \), i.e., `arg max P(X|O)`.

#### Example

- **Context**: Coin flip bias estimation.
- **Data**: Sequence of coin flips \( X = [1,0,0,0,0,1,0,0,1] \) where 1 represents heads and 0 represents tails.
- **Task**: Find the bias of the coin.
- **Assumption**: Coin flips are independent events.
- **Calculation**: 
  - \( P(X|O) = P(f = 1|O) \times \ldots \)
  - \( P(X|O) = O^3(1-O)^5 \)
- **Conclusion**: The data was most likely generated by a coin with a bias of \( O = \frac{3}{8} \).

